{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a70acf-6936-4e08-a7d6-d905b0b3fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Initialize the PPO agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_cartpole\")\n",
    "\n",
    "# Test the trained agent\n",
    "obs, _ = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eebf72-0f7d-4186-93e9-c3a717f84f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60227633-6235-4a49-9e3d-9a2fcaecdb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "obs, _ = env.reset()\n",
    "for _ in range(10):\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            total_rewards.append(episode_reward)\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "print(\"Episode rewards:\", total_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c289d-899b-4297-b21b-8f2f53375c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "obs, _ = env.reset()\n",
    "for _ in range(10):\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            total_rewards.append(episode_reward)\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "print(\"Episode rewards:\", total_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113c87c-3895-4743-bafd-89b513a09e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/ppo_cartpole_v1_maxed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b1872-7a69-4d50-a7c6-830e19fbdff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"models/ppo_cartpole_v1_maxed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf0547c-7f0b-4cce-8831-ff3e62734cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4971277-e6a3-4c37-8d27-133535332275",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e106ec0-9438-41e9-97d1-b96664f568f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abbbc8-13c1-400c-a431-a71a0cf54113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "obs, _ = env.reset()\n",
    "print(\"Environment loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21bc14-5346-43bf-aaab-e9188865e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# Initialize the PPO agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9996ac39-e2ba-463a-81fe-80dc5a56e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "total_reward = 0\n",
    "for _ in range(200):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(\"Total reward:\", total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851926f-5cb6-415a-8477-8079f02cf8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=100_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53033105-2fb2-4f1e-aaf2-225f5b60c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=100_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7daa8d-6bbe-49a0-b6d4-336efd5c01b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "total_reward = 0\n",
    "for _ in range(200):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(\"Total reward:\", total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa300c7-26f4-4c0c-a344-2f930ef082f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=200_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae4787a-4195-451c-a29c-2cfb692ffa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=200_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128bcb3a-0ba9-4898-a67b-858be37e7ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "for i in range(10):\n",
    "    model.learn(total_timesteps=20000, reset_num_timesteps=False)\n",
    "    \n",
    "    # Evaluate after each chunk\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    for _ in range(200):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    rewards.append(total_reward)\n",
    "    print(f\"Iteration {i+1}, Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bdcac-d3ec-4d8a-8f01-6e605a69ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    ent_coef=0.05,         # Higher entropy = more exploration\n",
    "    learning_rate=0.0003,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4c961-13bf-45ca-97dd-f19a27d86c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=200_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34afe542-79b0-402c-9fcf-100fc1947666",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/ppo_mountaincar_baseline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b56ad1-e269-4118-a7bd-c78b413707dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=100_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e0484-46af-4cbd-8b20-4799fdebbfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "total_reward = 0\n",
    "for _ in range(500):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(\"Total reward:\", total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a8a6a0-08b8-4e6c-9e20-873c6dc9e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/ppo_cartpole_1M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15d5e4-0409-4ff6-9cd2-998431d56a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"D:/RL_Projects/models/ppo_cartpole_1M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d97bd28-09a6-4684-8435-0af7099a9ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:/RL_Projects/logs/training_log_cartpole_1M.txt\", \"w\") as f:\n",
    "    f.write(\"Final reward: 500.0\\nTrained for 1M timesteps\\nEnvironment: CartPole-v1\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb41c1b3-9596-45ba-b515-c67defec6424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(\"D:/RL_Projects/logs\", exist_ok=True)\n",
    "\n",
    "# Now write the log file\n",
    "with open(\"D:/RL_Projects/logs/training_log_cartpole_1M.txt\", \"w\") as f:\n",
    "    f.write(\"Final reward: 500.0\\nTrained for 1M timesteps\\nEnvironment: CartPole-v1\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9d57c-acf1-47ce-a1ce-4124b88cc36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs, _ = env.reset()\n",
    "\n",
    "for _ in range(500):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    time.sleep(0.02)  # Slow down for visibility\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe7657c-7041-4807-b15d-503ac6fd320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"gymnasium[classic-control]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f234631-7159-48cb-b1dd-1b706ba28395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs, _ = env.reset()\n",
    "\n",
    "for _ in range(500):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    time.sleep(0.02)  # Slow down for visibility\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c8ee4-4315-4eb5-ba11-50f026fdf27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7205f1-b162-45d7-b587-dbe3815d3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"models/ppo_cartpole_1M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a67dd6-252e-4a50-8239-7bddfef86513",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4a2de5-0bac-4469-a4ce-a64025d61677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "obs, _ = env.reset()\n",
    "for _ in range(500):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    time.sleep(0.02)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd991a9e-bd25-4153-9ab3-53f6378ceea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df7282-5666-4ca3-a9ff-31f0d83ce377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "obs, _ = env.reset()\n",
    "for _ in range(500):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    time.sleep(0.02)  # Slow down for visibility\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860632ab-7742-43a3-9ec8-e794042766ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "obs, _ = env.reset()\n",
    "for _ in range(1000):  # Longer loop\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    time.sleep(0.02)\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31187aa8-4a03-4987-91a8-f10278468fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Reload model\n",
    "model = PPO.load(\"models/ppo_cartpole_1M\")\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Run multiple episodes\n",
    "for episode in range(5):\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(500):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        time.sleep(0.02)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ef4dbe-15f2-4825-8769-287e02031f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Reload model\n",
    "model = PPO.load(\"models/ppo_cartpole_1M\")\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Run multiple episodes\n",
    "for episode in range(5):\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(500):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        time.sleep(0.02)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb38b1-067d-470a-84b5-e644f99cc2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Reload model\n",
    "model = PPO.load(\"models/ppo_cartpole_1M\")\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Run multiple episodes\n",
    "for episode in range(5):\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(500):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        time.sleep(0.02)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2a679-15b5-4ac9-8c26-2f47ce17c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Reload model\n",
    "model = PPO.load(\"models/ppo_cartpole_1M\")\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Run multiple episodes\n",
    "for episode in range(5):\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(500):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        time.sleep(0.02)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8426d0c-2ee4-4ffb-a0ec-37020f124162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Reload model\n",
    "model = PPO.load(\"models/ppo_cartpole_1M\")\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Run multiple episodes\n",
    "for episode in range(5):\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(500):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        time.sleep(0.02)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c38f2-49a8-4b8c-ab59-ae9c6cc04151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28f293d3-79cf-446a-a153-5a8bf3c59148",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m action, _ = model.predict(obs)\n\u001b[32m      3\u001b[39m obs, reward, terminated, truncated, info = env.step(action)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtime\u001b[49m.sleep(\u001b[32m0.02\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "for _ in range(500):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    time.sleep(0.02)\n",
    "    if terminated or truncated:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba06cb79-3c8b-41ef-aa13-80320bf35194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbdb9ba8-2d35-4f58-8b1a-c0ddc1d6da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import gymnasium as gym\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a03cbe77-9139-40dd-b626-d5f4eead3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d0a9969-db02-4536-a4ee-3c3ce343c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22b53a2e-57db-460d-a1ef-f27267e67709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goura\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "obs, _ = env.reset()\n",
    "for _ in range(500):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    time.sleep(0.02)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cebabe4a-93cd-44ea-ad95-7ae298184d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4618430-8858-4fe3-a1c6-ea5926aa653c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.7     |\n",
      "|    ep_rew_mean     | 23.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 23.7        |\n",
      "|    ep_rew_mean          | 23.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008844518 |\n",
      "|    clip_fraction        | 0.0909      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.016       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.72        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 57.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.2        |\n",
      "|    ep_rew_mean          | 34.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010668559 |\n",
      "|    clip_fraction        | 0.0767      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.668      |\n",
      "|    explained_variance   | 0.0145      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 30.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 47.3        |\n",
      "|    ep_rew_mean          | 47.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008584514 |\n",
      "|    clip_fraction        | 0.0838      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.639      |\n",
      "|    explained_variance   | 0.226       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 53.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 61.4        |\n",
      "|    ep_rew_mean          | 61.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 221         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008502054 |\n",
      "|    clip_fraction        | 0.0695      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.611      |\n",
      "|    explained_variance   | 0.278       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.2        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 67.8        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "model.save(\"ppo_cartpole_10k\")\n",
    "model = PPO.load(\"ppo_cartpole_10k\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "for _ in range(500):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    time.sleep(0.02)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb063456-bb4c-458e-92d6-205b3496f72b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:311\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    301\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[32m    302\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    308\u001b[39m ) -> SelfOnPolicyAlgorithm:\n\u001b[32m    309\u001b[39m     iteration = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     total_timesteps, callback = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m     callback.on_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:422\u001b[39m, in \u001b[36mBaseAlgorithm._setup_learn\u001b[39m\u001b[34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;66;03m# Avoid resetting the environment when calling ``.learn()`` consecutive times\u001b[39;00m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    423\u001b[39m     \u001b[38;5;28mself\u001b[39m._last_obs = \u001b[38;5;28mself\u001b[39m.env.reset()  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28mself\u001b[39m._last_episode_starts = np.ones((\u001b[38;5;28mself\u001b[39m.env.num_envs,), dtype=\u001b[38;5;28mbool\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c616095-ef34-4007-b4bc-ffc56a4181a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb32b22d-fecb-4c35-9ce6-4e5c0009be7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PPO.__init__() missing 1 required positional argument: 'env'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMlpPolicy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: PPO.__init__() missing 1 required positional argument: 'env'"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4604a00a-422b-4df4-b5fb-c337286aed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create environment with rendering\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Create PPO model with environment\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9c644a-eaae-4034-9feb-a18ef6774726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23       |\n",
      "|    ep_rew_mean     | 23       |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 25.7        |\n",
      "|    ep_rew_mean          | 25.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009550773 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.00237     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 56.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.7        |\n",
      "|    ep_rew_mean          | 36.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009659607 |\n",
      "|    clip_fraction        | 0.0686      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.664      |\n",
      "|    explained_variance   | 0.0494      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 37          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.4        |\n",
      "|    ep_rew_mean          | 49.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008278167 |\n",
      "|    clip_fraction        | 0.0695      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 56.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 63.6         |\n",
      "|    ep_rew_mean          | 63.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 221          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067065437 |\n",
      "|    clip_fraction        | 0.0589       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.605       |\n",
      "|    explained_variance   | 0.259        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.4         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0138      |\n",
      "|    value_loss           | 67.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1c1d7c41590>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895317a3-b798-4695-9e2f-3ed73e514ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_cartpole_10k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf92f41-2406-4ab7-885b-c47331a2df84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create environment with rendering\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Create PPO model with environment\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c14649-81d1-4556-ac6a-e9a0f54edb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "for _ in range(500):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    time.sleep(0.02)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20479de2-cae6-4b1f-8fa0-a778c9d33d3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m obs, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m500\u001b[39m):\n\u001b[32m      3\u001b[39m     action, _ = model.predict(obs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:146\u001b[39m, in \u001b[36mTimeLimit.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[32m    137\u001b[39m \n\u001b[32m    138\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \u001b[33;03m    The reset environment\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28mself\u001b[39m._elapsed_steps = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\core.py:333\u001b[39m, in \u001b[36mWrapper.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\n\u001b[32m    330\u001b[39m     \u001b[38;5;28mself\u001b[39m, *, seed: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, options: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    331\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    332\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:400\u001b[39m, in \u001b[36mOrderEnforcing.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[32m    399\u001b[39m \u001b[38;5;28mself\u001b[39m._has_reset = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\core.py:333\u001b[39m, in \u001b[36mWrapper.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\n\u001b[32m    330\u001b[39m     \u001b[38;5;28mself\u001b[39m, *, seed: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, options: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    331\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    332\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:295\u001b[39m, in \u001b[36mPassiveEnvChecker.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_reset_passive_checker(\u001b[38;5;28mself\u001b[39m.env, seed=seed, options=options)\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:244\u001b[39m, in \u001b[36mCartPoleEnv.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28mself\u001b[39m.steps_beyond_terminated = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(\u001b[38;5;28mself\u001b[39m.state, dtype=np.float32), {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:334\u001b[39m, in \u001b[36mCartPoleEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    331\u001b[39m gfxdraw.hline(\u001b[38;5;28mself\u001b[39m.surf, \u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.screen_width, carty, (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m))\n\u001b[32m    333\u001b[39m \u001b[38;5;28mself\u001b[39m.surf = pygame.transform.flip(\u001b[38;5;28mself\u001b[39m.surf, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscreen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mblit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    336\u001b[39m     pygame.event.pump()\n",
      "\u001b[31merror\u001b[39m: display Surface quit"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "for _ in range(500):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    time.sleep(0.02)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0728d427-493c-4be7-a3a4-15afb28121e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      2\u001b[39m time.sleep(\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m obs, _ = \u001b[43menv\u001b[49m.reset()\n",
      "\u001b[31mNameError\u001b[39m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time.sleep(1)\n",
    "obs, _ = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73cb10d3-cef5-4ec2-9b11-e9793e0b57d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1938593a-eda0-45ab-afc7-5474dab40cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee322b0-833f-4d8f-b1f2-f37bace2a316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
